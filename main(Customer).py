# -*- coding: utf-8 -*-
"""Customer Segmentation(internship).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a9mj4VVtdCy3Vo5FftXK4iWeue0n4kjO
"""

# Customer Segmentation: Mall Customers (Annual Income vs Spending Score)
# Requirements: pandas, numpy, matplotlib, scikit-learn
# Run in Jupyter or Colab for immediate visuals.

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA

# --- 1) Load dataset (or synthesize if not present) ---
# Put your Mall_Customers.csv in the working dir (common Kaggle file name is "Mall_Customers.csv")
possible_paths = ["/content/Mall_Customers.csv", "Mall_Customers.csv", "/mnt/data/Mall_Customers.csv", "mall_customers.csv"]
df = None
for p in possible_paths:
    if os.path.exists(p):
        df = pd.read_csv(p)
        print(f"Loaded dataset from: {p}")
        break

if df is None:
    print("No CSV found â€” creating a synthetic Mall-Customers-like dataset.")
    np.random.seed(42)
    n = 200
    genders = np.random.choice(['Male', 'Female'], size=n)
    ages = np.random.randint(18, 70, size=n)
    # income clusters
    income_low = np.random.normal(25, 6, 60)
    income_mid = np.random.normal(55, 8, 80)
    income_high = np.random.normal(90, 7, 60)
    income = np.concatenate([income_low, income_mid, income_high])
    np.random.shuffle(income)
    spending = np.concatenate([
        np.random.normal(70, 10, 60),
        np.random.normal(40, 12, 80),
        np.random.normal(20, 8, 60)
    ])
    spending = np.clip(spending, 1, 99)
    np.random.shuffle(spending)
    df = pd.DataFrame({
        'CustomerID': np.arange(1, n+1),
        'Gender': genders,
        'Age': ages,
        'Annual Income (k$)': np.round(income, 1),
        'Spending Score (1-100)': np.round(spending, 1)
    })

# Quick peek
print(df.head())

# --- 2) Select features and scale ---
features = ['Annual Income (k$)', 'Spending Score (1-100)']
X = df[features].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# --- 3) Visual exploration (raw scatter) ---
plt.figure(figsize=(7,5))
plt.scatter(df['Annual Income (k$)'], df['Spending Score (1-100)'], s=40)
plt.title("Annual Income vs Spending Score (raw values)")
plt.xlabel("Annual Income (k$)")
plt.ylabel("Spending Score (1-100)")
plt.grid(alpha=0.25)
plt.show()

# --- 4) Find optimal k with Elbow & Silhouette (k from 2..10) ---
ks = list(range(2, 11))
inertias = []
sil_scores = []

for k in ks:
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = km.fit_predict(X_scaled)
    inertias.append(km.inertia_)
    sil_scores.append(silhouette_score(X_scaled, labels))

# Plot elbow (inertia)
plt.figure(figsize=(6.5,4))
plt.plot(ks, inertias, marker='o')
plt.title("Elbow method (inertia vs k)")
plt.xlabel("k"); plt.ylabel("Inertia"); plt.grid(alpha=0.25); plt.show()

# Plot silhouette
plt.figure(figsize=(6.5,4))
plt.plot(ks, sil_scores, marker='o')
plt.title("Silhouette score vs k")
plt.xlabel("k"); plt.ylabel("Silhouette score"); plt.grid(alpha=0.25); plt.show()

best_k = ks[int(np.argmax(sil_scores))]
print(f"Suggested best k by silhouette: {best_k} (silhouette={max(sil_scores):.3f})")

# --- 5) Fit final KMeans and visualize clusters (original scale) ---
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=20)
labels_km = kmeans.fit_predict(X_scaled)
centers_orig = scaler.inverse_transform(kmeans.cluster_centers_)

plt.figure(figsize=(7,5))
for c in np.unique(labels_km):
    mask = labels_km == c
    plt.scatter(X[mask,0], X[mask,1], s=40, label=f"Cluster {c}")
plt.scatter(centers_orig[:,0], centers_orig[:,1], s=200, marker='X')  # cluster centers
plt.title(f"KMeans clusters (k={best_k})")
plt.xlabel("Annual Income (k$)"); plt.ylabel("Spending Score (1-100)")
plt.legend(); plt.grid(alpha=0.25); plt.show()

# Attach labels and show cluster averages
df_km = df.copy()
df_km['kmeans_cluster'] = labels_km
summary = df_km.groupby('kmeans_cluster').agg(
    size=('CustomerID','count'),
    avg_income_k=('Annual Income (k$)','mean'),
    avg_spending_score=('Spending Score (1-100)','mean'),
    avg_age=('Age','mean')
).reset_index().sort_values('kmeans_cluster')
print("\nKMeans cluster summary (averages):")
print(summary)

# --- Bonus: DBSCAN on the scaled two features ---
db = DBSCAN(eps=0.6, min_samples=5)  # eps in scaled space; adjust if needed
labels_db = db.fit_predict(X_scaled)
df_db = df.copy()
df_db['dbscan_cluster'] = labels_db
summary_db = df_db.groupby('dbscan_cluster').agg(
    size=('CustomerID','count'),
    avg_income_k=('Annual Income (k$)','mean'),
    avg_spending_score=('Spending Score (1-100)','mean')
).reset_index().sort_values('dbscan_cluster')
print("\nDBSCAN cluster summary (label -1 is noise):")
print(summary_db)

# --- Optional: PCA projection (useful if you later include more features) ---
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)
plt.figure(figsize=(7,5))
for c in np.unique(labels_km):
    mask = labels_km == c
    plt.scatter(X_pca[mask,0], X_pca[mask,1], s=40, label=f"Cluster {c}")
plt.title("KMeans clusters visualized with PCA projection")
plt.xlabel("PCA component 1"); plt.ylabel("PCA component 2")
plt.legend(); plt.grid(alpha=0.25); plt.show()